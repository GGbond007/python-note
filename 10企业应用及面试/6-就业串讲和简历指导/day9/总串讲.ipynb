{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 知识点总结\n",
    "- requests模块\n",
    "    - 爬取数据流程：\n",
    "        - 指定url\n",
    "        - 发起请求\n",
    "        - 获取响应数据\n",
    "        - 数据解析\n",
    "        - 持久化存储\n",
    "    - get，post作用：\n",
    "        - 发起网络请求\n",
    "    - get，post参数：\n",
    "        - url\n",
    "        - headers：请求头的伪装\n",
    "        - data/params：参数动态化\n",
    "        - proxies：代理\n",
    "    - 动态加载的数据：\n",
    "        - 产生的机制：\n",
    "            - ajax\n",
    "            - js\n",
    "        - 如何检测爬取的数据是否为动态加载\n",
    "            - 方案：基于抓包工具进行局部搜索\n",
    "        - 如何捕获动态加载数据？\n",
    "            - 抓包工具进行全局搜索\n",
    "            - 注意：如果基于抓包工具进行全局搜索，搜索不到说明动态加载的数据是加密了。\n",
    "    - 模拟登陆：\n",
    "        - 发起post请求。\n",
    "    - 打码平台使用：\n",
    "        - 超级鹰\n",
    "        - 云打码\n",
    "    - cookie的处理：\n",
    "        - 方式1：手动处理\n",
    "        - 方式2：自动处理。Session对象。\n",
    "    - 代理ip：\n",
    "        - 代理池。\n",
    "    - 动态变化的请求参数：\n",
    "        - 一般会被隐藏在前台页面源码中\n",
    "        - 抓包工具全局搜索\n",
    "    - js加密处理\n",
    "        - js逆向\n",
    "            - 手动：找出解密实现方法（js函数）,手动改写成python代码，执行获取返回值\n",
    "            - 自动：pyexecjs可以直接模拟执行js代码\n",
    "        - js混淆\n",
    "            - 网站会将重要的相关代码进行加密。\n",
    "        - js反混淆\n",
    "            - 暴力破解的方式将加密的代码进行解密\n",
    "    - 异步爬虫\n",
    "        - 线程池：\n",
    "        - 多任务的异步协程\n",
    "            - 特殊的函数\n",
    "            - 协程对象：协程 == 特殊的函数 == 一组指定操作\n",
    "            - 任务对象：高级协程对象。任务==协程==特殊的函数==一组指定操作\n",
    "                - 如何给任务对象绑定回调函数\n",
    "                    - 注意：回调函数一定是在任务对象对应的这组操作执行完毕后才会执行回调函数\n",
    "            - 事件循环对象\n",
    "                - 可以将多个任务对象注册到事件循环中，如果该对象开启后，则内部注册的多个任务对象对应的多组指定操作就会被异步执行。\n",
    "            - 注意【重要】：在特殊函数实现内部不可以出现不支持异步模块的代码，否则会中断整个异步效果。\n",
    "    - aiohttp基于异步的网络请求模块。\n",
    "    - 基于请求发送的模块：\n",
    "        - requests\n",
    "        - aiohttp\n",
    "        - urllib\n",
    "- 数据解析\n",
    "    - 正则表达式：\n",
    "    - bs4：\n",
    "    - xpath：xpath表达式！\n",
    "        - 在xpath表达式中不可以出现tbody标签\n",
    "- selenium\n",
    "    - 作用：\n",
    "        - 便捷的捕获到动态加载的数据（可见即可得）\n",
    "        - 实现模拟登录\n",
    "    - 规避selenium被检测的方式\n",
    "        - selenium托管\n",
    "    - 使用流程：\n",
    "    - find系列函数：\n",
    "    - switch_to.frame函数\n",
    "    - 动作链\n",
    "    - phantomJs：\n",
    "    - 谷歌无头浏览器：\n",
    "- scrapy\n",
    "    - 项目创建流程：\n",
    "        - scrapy startproject ProName\n",
    "        - cd ProName\n",
    "        - scrapy genspider spiderName www.xxx.com\n",
    "        - scrapy crawl spiderName\n",
    "    - scrapy的数据解析：\n",
    "        - response.xpath('xpath表达式')\n",
    "        - lxml.etree中对应的解析的区别：\n",
    "            - scrapy中进行xpath解析，定位到的标签或者数据，都是存在Selector对象中。需要使用extract系列的函数将Selector对象中的数据取出。\n",
    "    - 持久化存储：\n",
    "        - 方式1：基于终端指令的形式\n",
    "            - 可以直接将parse方法的返回值写入指定后缀的文本文件中。\n",
    "        - 方式2：基于管道的形式\n",
    "            - 在爬虫文件中进行数据解析\n",
    "            - 将解析到的数据存储到item类型的对象中\n",
    "            - 将item对象提交给管道\n",
    "            - 管道中接受item对象，将其进行任意形式的持久化存储\n",
    "            - 在配置文件中开启管道\n",
    "            - 注意：\n",
    "                - 一个管道类表示一种持久化存储的形式（爬虫文件向管道提交的item只会提交给优先级最高的那一个管道类）\n",
    "                - 管道类中的process_item方法中返回值return item的作用:\n",
    "                    - 将item传递给下一个即将被执行的管道类\n",
    "            - 管道类中还会重写父类的两个方法：\n",
    "                - open_spider\n",
    "                - close_spider\n",
    "                    - 都是在爬虫开始和结束的时候各自被调用一次，而process_item方法会被调用多次。\n",
    "    - 处理分页数据爬取：\n",
    "        - 可以将url写入到start_urls列表中，则就可以对其进行请求发送：自动请求发送\n",
    "        - 手动请求发送：通过调用scrapy中的Request(url,callback)进行指定url的手动请求发送\n",
    "        - 请求方法调用前加上yield关键字\n",
    "    - post请求\n",
    "        - FormRequest()\n",
    "    - cookie处理：\n",
    "        - 配置文件中：COOKIE_ENABLE = True\n",
    "    - 日志等级：\n",
    "        - LOG_LEVEL = ‘ERROR’\n",
    "    - 请求传参：\n",
    "        - 作用：实现深度爬取（爬取的数据没有在同一张页面中）\n",
    "        - 将item对象传递给其他的解析方法。\n",
    "            - yield scrapy.Request(url,callback,mata={'item':item})将meta传递给callback\n",
    "            - 在callback中通过response.meta接受传递过来的meta对象\n",
    "    - 五大核心组件原理：\n",
    "        - SPider，引擎，调度器，管道，下载器\n",
    "    - 下载中间件：\n",
    "        - 下载中间件\n",
    "            - 拦截所用的请求和响应\n",
    "            - 拦截请求\n",
    "                - 进行请求头的修改\n",
    "                - 代理设置\n",
    "            - 拦截响应：\n",
    "                - 响应数据进行篡改\n",
    "    - selenium在scrapy中的应用：\n",
    "    - crawlSpider：\n",
    "        - 实现全站数据爬取\n",
    "        - 链接提取器\n",
    "        - 规则解析器\n",
    "- 分布式\n",
    "    - scrapy为何不能实现分布式：\n",
    "        - 调度器，管道不能共享\n",
    "    - scrapy-redis的作用：\n",
    "        - 可以给scrapy提供可以被共享的管道和调度器\n",
    "    - 实现分布式的方式：\n",
    "    - 流程：\n",
    "- 增量式爬虫\n",
    "    - 作用：监测网站数据更新的情况。\n",
    "    - 实现的核心：记录表（redis的set）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例总结\n",
    "    - 爬取肯德基餐厅位置信息：http://www.kfc.com.cn/kfccda/index.aspx\n",
    "    - 爬取药监总局：http://125.35.6.84:81/xk/\n",
    "    - 爬取糗事百科图片：https://www.qiushibaike.com/pic/\n",
    "    - 下载免费简历模板：http://sc.chinaz.com/jianli/free.html\n",
    "    - 解析城市名称：https://www.aqistudy.cn/historydata/\n",
    "    - 古诗文网：https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx\n",
    "    - 网易新闻：https://news.163.com/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反爬机制总结\n",
    "- robots.txt\n",
    "- UA检测\n",
    "- 验证码\n",
    "- 图片懒加载\n",
    "- cookie\n",
    "- 禁IP\n",
    "- 动态token\n",
    "- 数据动态加载\n",
    "- js加密\n",
    "- js混淆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 面试题\n",
    "- 如何提升爬虫的效率\n",
    "    - 线程池\n",
    "    - scrapy配置文件相关配置（禁用cookie，禁止重试，减小下载超时，增加并发，日志等级）\n",
    "- scrapy核心组件工作流程\n",
    "- scrapy中如何设置代理（两种方法）\n",
    "    - 中间件\n",
    "    - 环境变量（os.environ['HTTPS_PROXY'] = 'https:ip:port'）\n",
    "- scrapy如何实现限速\n",
    "    - DOWNLOAD_DELAY = 1\n",
    "- scrapy如何实现暂停爬虫\n",
    "    - JOBDIR='sharejs.com'\n",
    "    - control-C\n",
    "- pipeline如何丢弃一个item对象\n",
    "- scrapy-redis组件作用\n",
    "- 实现深度和广度优先:默认为深度优先。\n",
    "    - DEPTH_PRIORITY = 1\n",
    "    - SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'\n",
    "    - SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'\n",
    "    - 深度优先：不全部保留结点，占用空间少；运行速度慢\n",
    "    - 广度优先：保留全部结点，占用空间大；运行速度快\n",
    "    - https://www.cnblogs.com/zhaof/p/7092400.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
